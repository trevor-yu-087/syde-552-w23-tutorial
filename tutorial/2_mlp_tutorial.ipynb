{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron activity\n",
    "\n",
    "In this activity, you will make a multi-layer perceptron (MLP) model in the PyTorch deep learning package to perform classification of hand-written digits in the classic MNIST dataset.\n",
    "\n",
    "There are 5 tasks for you to complete the example. Cells have clearly marked `# TODO` and `#####` comments for you to insert your code between. Variables assigned to `None` should keep the same name but assigned to their proper implementation.\n",
    "\n",
    "1. Complete the implementation of the MLP model by completing the `__init__` and `forward` functions.\n",
    "2. Setup the optimizer and loss function for training.\n",
    "3. Fill in the missing steps in the train and test loop.\n",
    "4. Train the model for 5 epochs.\n",
    "5. Visualize the model predictions on some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to import relevant packages\n",
    "\n",
    "import torch  # Main torch import for torch tensors (arrays)\n",
    "import torch.nn as nn  # Neural network module for building deep learning models\n",
    "import torch.nn.functional as F  # Functional module, includes activation functions\n",
    "import torch.optim as optim  # Optimization module\n",
    "import torchvision  # Vision / image processing package built on top of torch\n",
    "\n",
    "from matplotlib import pyplot as plt  # Plotting and visualization\n",
    "from sklearn.metrics import accuracy_score  # Computing accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to download the data and setup the pre-processing pipeline\n",
    "\n",
    "# Common practice to normalize input data to neural networks (0 mean, unit variance)\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),  # All inputs to PyTorch neural networks must be torch.Tensor\n",
    "    torchvision.transforms.Normalize(mean=0.1307, std=0.3081)  # Subtracts mean and divides by std. Note that the raw data is between [0, 1]\n",
    "])\n",
    "\n",
    "# Download the MNIST data and lazily apply the transformation pipeline\n",
    "train_data = torchvision.datasets.MNIST('./datafiles/', train=True, download=True, transform=transform)\n",
    "test_data = torchvision.datasets.MNIST('./datafiles/', train=False, download=True, transform=transform)\n",
    "\n",
    "# Setup data loaders\n",
    "# Note: Iterating through the dataloader yields batches of (inputs, targets)\n",
    "# where inputs is a torch.Tensor of shape (batch, 28, 28) and targets is a torch.Tensor of shape (batch,)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to visualize 20 examples from the test dataset\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(5, 6))\n",
    "\n",
    "plot_images = []\n",
    "plot_labels = []\n",
    "\n",
    "for i, ax in enumerate(axs.flatten(), start=1000):\n",
    "    (image, label) = test_data[i]\n",
    "\n",
    "    # Save this data for later\n",
    "    plot_images.append(image)\n",
    "    plot_labels.append(label)\n",
    "\n",
    "    # Plot each image\n",
    "    ax.imshow(image.squeeze(), cmap=\"viridis\")\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plot_images = torch.cat(plot_images)  # Combine all the images into a single batch for later\n",
    "\n",
    "print(f\"Each image is a torch.Tensor and has shape {image.shape}.\")\n",
    "print(f\"The labels are the integers 0 to 9, representing the digits.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Complete the implementation of the MLP model\n",
    "\n",
    "Although we draw diagrams of hidden layers as neurons with incoming and outgoing connections, in practice, we implement this with two linear layers (also called \"dense layers\") and a pointwise non-linearity in between. The first layer is a linear transform (matrix multiplication) from the input dimension to the hidden dimension. The second layer is a linear transform from the hidden dimension to the output dimension.\n",
    "\n",
    "For this model, the input dimension is (28*28) as we will flatten the 2D images into a 1D vector. The hidden dimension is 100. The output dimension is 10, since we have 10 classes. We will use the ReLU non-linearity.\n",
    "\n",
    "In PyTorch, a model is defined by subclassing the `nn.Module` class and we define behaviour in two methods. In the `__init__` method, we setup the model architecture such as number layers, the size of each layer, etc. In the `forward()` method, we define the operations performed by the model's layers on the input data to produce outputs.\n",
    "\n",
    "Note: You do not need to apply a softmax to the outputs as this is automatically done with the appropriate loss function.\n",
    "\n",
    "Relevant documentation:\n",
    "\n",
    "- [PyTorch nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)\n",
    "\n",
    "- [PyTorch activation functions](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions)\n",
    "\n",
    "- [PyTorch linear layer documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Assign self.hidden to a torch linear layer of the correct size\n",
    "        self.hidden = None\n",
    "        # TODO: Assign self.output to a torch linear layer of the correct size\n",
    "        self.output = None\n",
    "        #####\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass implementation for the network\n",
    "        \n",
    "        :param x: torch.Tensor of shape (batch, 28, 28), input images\n",
    "\n",
    "        :returns: torch.Tensor of shape (batch, 10), output logits\n",
    "        \"\"\"\n",
    "        x = torch.flatten(x, 1)  # shape (batch, 28*28)\n",
    "        # TODO: Process x through self.hidden, relu, and self.output and return the result\n",
    "\n",
    "        return None\n",
    "        #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to test your implementation. You should expect an output tensor of shape (2, 10)\n",
    "\n",
    "mlp = MultiLayerPerceptron()\n",
    "x = torch.randn(2, 28, 28)\n",
    "z = mlp(x)\n",
    "z.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup optimizer and loss function\n",
    "\n",
    "The current standard optimizer in deep learning is the Adam optimizer. Use a learning rate of $1\\times 10^{-2}$. \n",
    "\n",
    "The task we are performing is multiclass classification (10 independent classes, one for each digit). The loss function to use for this task is cross entropy loss.\n",
    "\n",
    "Relevant documentation:\n",
    "- [PyTorch optimizers](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "- [PyTorch loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate your model and setup the optimizer\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Setup the cross entropy loss function\n",
    "\n",
    "#####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fill in the missing steps of the train and test loop\n",
    "\n",
    "During the training loop, we perform the following steps:\n",
    "\n",
    "1. Fetch the next batch of inputs and targets from the dataloader\n",
    "2. Zero the parameter gradients\n",
    "3. Compute the model output predictions from the inputs\n",
    "4. Compute the loss between the model outputs and the targets\n",
    "5. Compute the parameter gradients with backpropagation\n",
    "6. Perform a gradient descent step with the optimizer to update the model parameters\n",
    "\n",
    "Relevant documentation:\n",
    "- [PyTorch optimization step](https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer, epoch=-1):\n",
    "    \"\"\"\n",
    "    Trains a model for one epoch (one pass through the entire training data).\n",
    "\n",
    "    :param model: PyTorch model\n",
    "    :param train_loader: PyTorch Dataloader for training data\n",
    "    :param loss_fn: PyTorch loss function\n",
    "    :param optimizer: PyTorch optimizer, initialized with model parameters\n",
    "    :kwarg epoch: Integer epoch to use when printing loss and accuracy\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model.train()  # Set model in training mode\n",
    "    for i, (inputs, targets) in enumerate(train_loader):  # 1. Fetch next batch of data\n",
    "        # TODO: Fill in the rest of the training loop\n",
    "        # 2. Zero parameter gradients\n",
    "        outputs = None  # 3. Compute model outputs\n",
    "        loss = None  # 4. Compute loss between outputs and targets\n",
    "        loss.backward()  # 5. Backpropagation for parameter gradients\n",
    "        # 6. Gradient descent step\n",
    "        #####\n",
    "\n",
    "        # Track some values to compute statistics\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=-1)  # Take the class with the highest output as the prediction\n",
    "        all_predictions.extend(preds.tolist())\n",
    "        all_targets.extend(targets.tolist())\n",
    "\n",
    "        # Print some statistics every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            running_loss = total_loss / (i + 1)\n",
    "            print(f\"Epoch {epoch + 1}, batch {i + 1}: loss = {running_loss:.2f}\")\n",
    "\n",
    "    # TODO: Compute the overall accuracy        \n",
    "    acc = None\n",
    "    #####\n",
    "\n",
    "    # Print average loss and accuracy\n",
    "    print(f\"Epoch {epoch + 1} done. Average train loss = {total_loss / len(train_loader):.2f}, average train accuracy = {acc * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss_fn, epoch=-1):\n",
    "    \"\"\"\n",
    "    Tests a model for one epoch of test data.\n",
    "\n",
    "    Note:\n",
    "        In testing and evaluation, we do not perform gradient descent optimization, so steps 2, 5, and 6 are not needed.\n",
    "        For performance, we also tell torch not to track gradients by using the `with torch.no_grad()` context.\n",
    "\n",
    "    :param model: PyTorch model\n",
    "    :param test_loader: PyTorch Dataloader for test data\n",
    "    :param loss_fn: PyTorch loss function\n",
    "    :kwarg epoch: Integer epoch to use when printing loss and accuracy\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model.eval()  # Set model in evaluation mode\n",
    "    for i, (inputs, targets) in enumerate(test_loader):  # 1. Fetch next batch of data\n",
    "        with torch.no_grad():\n",
    "            # TODO: Compute the model outputs and loss only. Do not update using the optimizer\n",
    "            outputs = None  # 3. Compute model outputs\n",
    "            loss = None  # 4. Compute loss between outputs and targets\n",
    "            #####\n",
    "\n",
    "            # Track some values to compute statistics\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=-1)  # Take the class with the highest output as the prediction\n",
    "            all_predictions.extend(preds.tolist())\n",
    "            all_targets.extend(targets.tolist())\n",
    "\n",
    "    # TODO: Compute the overall accuracy        \n",
    "    acc = None\n",
    "    #####\n",
    "\n",
    "    # Print average loss and accuracy\n",
    "    print(f\"Epoch {epoch + 1} done. Average test loss = {total_loss / len(test_loader):.2f}, average test accuracy = {acc * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the model for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy the setup for the model, optimizer, and loss function from Section 2 to here\n",
    "# Then, run this cell to train the model for 5 epochs\n",
    "model = None\n",
    "\n",
    "#####\n",
    "\n",
    "for epoch in range(5):\n",
    "    # TODO: Fill in the rest of the arguments to the train and test functions\n",
    "    train(model, ..., epoch=epoch)\n",
    "    test(model, ..., epoch=epoch)\n",
    "    #####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visually compare the model predictions\n",
    "\n",
    "We will lastly see the trained model's predictions on the 20 examples we visualized in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to visualize the data\n",
    "\n",
    "# Evaluate the model on the plot_images\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    plot_outputs = model(plot_images)\n",
    "    plot_preds = torch.argmax(plot_outputs, dim=-1)\n",
    "\n",
    "# Plot and show the labels\n",
    "fig, axs = plt.subplots(4, 5, figsize=(7, 8))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    image = plot_images[i]\n",
    "    label = plot_labels[i]\n",
    "    pred = plot_preds[i]\n",
    "\n",
    "    ax.imshow(image.squeeze(), cmap=\"viridis\")\n",
    "    ax.set_title(f\"Prediction: {pred}\\nLabel: {label}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fddaefb328894158b465f763a80d93613a8dda1c2f29f2bb5673421f61ac7a4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
