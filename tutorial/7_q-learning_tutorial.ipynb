{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement activity\n",
    "\n",
    "In this activity, you will make an agent that uses reinforcement learning to navigate the Cliff Walking environment. You will write methods in the agent class to implement Q-learning and an $\\epsilon$-greedy action selection strategy. \n",
    "\n",
    "There are 3 main tasks for you to complete and 2 tasks to plot the results of the RL model. Cells have clearly marked `# TODO` and `#####` comments for you to insert your code between. Variables assigned to `None` should keep the same name but assigned to their proper implementation.\n",
    "\n",
    "1. Complete the `get_action` method of the agent and implement an $\\epsilon$-greedy strategy\n",
    "2. Complete the `update` method of the agent and implement the temporal difference loss and update equation for Q-learning\n",
    "3. Complete the training loop for the RL model\n",
    "4. Plot the training metrics\n",
    "5. Plot the path and policy at various points of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this in Google Colab to install the RL gym package\n",
    "# !pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to import relevant packages\n",
    "\n",
    "import numpy as np  # Arrays and numerical computing\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import gymnasium as gym  # Reinforcement learning environments\n",
    "import random  # Random number generation\n",
    "\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "from matplotlib import patches\n",
    "from matplotlib import colormaps\n",
    "cmap = colormaps.get_cmap(\"jet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff Walking environment\n",
    "[Gymnasium documentation for cliff walking](https://gymnasium.farama.org/environments/toy_text/cliff_walking/#arguments)\n",
    "\n",
    "## Observation and state space\n",
    "The goal of the cliff walking environment is to cross a $4 \\times 12$ grid from the start point to the end point while avoiding falling off a cliff, which covers 10 tiles at the bottom of the grid. An observation is a representation of the current environment (e.g. position), while a state is a valid representation for the agent to be in. Note that not all observations are states (e.g. the agent cannot be on a cliff space), but all states are observations.\n",
    "\n",
    "The observation space contains 48 discrete locations at each point in the grid, although only 38 of them are valid states for the agent to be in: the 36 upper green tiles (states 0..35), the blue start space (state 36), and the red end space (state 47). Brown tiles (observations 37..46) represent the cliff and are not valid states for the agent. If the agent moves into a cliff tile, it returns to the start space. The agent starts in state 36 and the goal is to traverse the environment to state 47.\n",
    "\n",
    "## Action space\n",
    "The action space describes all possible actions the agent can take to interact with its environment. The action space for cliff walking contains 4 discrete actions, corresponding to the 4 directions the agent can move:\n",
    "  - 0: Move up ($\\uarr$)\n",
    "  - 1: Move right ($\\rarr$)\n",
    "  - 2: Move down ($\\darr$)\n",
    "  - 3: Move left ($\\larr$)\n",
    "\n",
    "## Reward\n",
    "The agent is rewarded -1 for every timestep spent in the environment. If the agent steps off the cliff, it incurrs a -100 reward. As we are trying to maximize the reward upon completion of the task, the agent should try to learn how to reach the end tile in a minimum number of time steps, thus taking the shortest path. As there is a steep penalty for falling off the cliff, the agent should learn not to do this too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to visualize the state space\n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "\n",
    "xs = np.arange(12)\n",
    "ys = np.arange(4)\n",
    "xs, ys = np.meshgrid(xs, ys, indexing=\"xy\")\n",
    "\n",
    "labels = np.arange(48)\n",
    "\n",
    "for x, y, label in zip(xs.flatten(), ys.flatten(), labels):\n",
    "    ax.text(x + 0.5, -y + 3.5, label, c='k', ha=\"center\", va=\"center\")\n",
    "\n",
    "ax.set_xticks(np.arange(0, 13))\n",
    "ax.set_yticks(np.arange(0, 4))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.grid(lw=1, c='k')\n",
    "\n",
    "# Add in color patches\n",
    "grass = patches.Rectangle(xy=(0, 1), width=13, height=3, fc=\"xkcd:grass green\", alpha=0.6, zorder=0)\n",
    "start = patches.Rectangle(xy=(0, 0), width=1, height=1, fc=\"xkcd:bright blue\", alpha=0.6, zorder=0)\n",
    "end = patches.Rectangle(xy=(11, 0), width=1, height=1, fc=\"xkcd:red\", alpha=0.75, zorder=0)\n",
    "cliff = patches.Rectangle(xy=(1, 0), width=10, height=1, fc=\"xkcd:dark brown\", zorder=3)\n",
    "for patch in [grass, start, end, cliff]:\n",
    "    ax.add_patch(patch)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_title(\"State space for Cliff Walking\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 and 2: Complete the agent implementation\n",
    "\n",
    "The main attribute of the `Agent` class we will interact with is the `self.q_values` array. The array is 48 x 4, and represents the possible actions ($a_t$) that could be taken in the given state ($s_t$). Recall the action-value function, $Q(s, a)$, represents the value of taking action $a$ in state $s$. We can get the value of $Q(s, a)$ by simply indexing `self.q_values[s, a]`. \n",
    "\n",
    "The temporal difference loss for Q-learning is as follows:\n",
    "\n",
    "$$\\mathcal{L_{td}} = r_t + \\gamma \\cdot \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)$$\n",
    "\n",
    "where $\\max_a Q(s_{t+1}, a)$ is the optimal future action value as computed by the maximal Q value for the next observation, $s_{t+1}$. \n",
    "\n",
    "The update rule for Q-learning is as follows, where $\\eta$ is the learning rate:\n",
    "$$Q^{\\text{new}}(s, a) = Q^{\\text{old}}(s, a) + \\eta \\cdot \\mathcal{L_{td}}$$\n",
    "\n",
    "$\\epsilon$-greedy action selection chooses a random action ($a_t \\in \\{0, 1, 2, 3\\}$) with probability $\\epsilon$, and choose the optimal action ($a_t = \\text{arg} \\max_{a} Q(s_t, a)$) otherwise.\n",
    "\n",
    "The RL gym environment deals with the logic of running the game, so you can assume that the `observation` values it passes to the agent are valid states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, learning_rate, epsilon, discount_factor=1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate: float, learning rate used in Q-learning update step\n",
    "        epsilon: float, between [0, 1), probability that the agent will take a random action\n",
    "        discount_factor: float, by default 1, the importance of future rewards \n",
    "        \"\"\"\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.n_observations = env.observation_space.n\n",
    "        self.q_values = np.zeros((self.n_observations, self.n_actions))  # 48 x 4\n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.td_history = []\n",
    "        self.q_history = {}\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        \"\"\"Epsilon-greedy action selection\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        observation: int, observation of the agent's state from the environment\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        action: int, selected action the agent will perform\n",
    "        \"\"\"\n",
    "        # TODO: Implement epsilon greedy sampling action sampling\n",
    "        action = None\n",
    "        #####\n",
    "        return action\n",
    "        \n",
    "    def update(self, observation, action, reward, terminated, next_observation):\n",
    "        \"\"\"Updates the Q values after taking an action in the environment\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        observation: int, obesrvation of the agent's state from the environment\n",
    "        action: int, action the agent selected to perform\n",
    "        reward: float, the reward of performing the action from the environment\n",
    "        terminated: bool, whether the episode has been terminated by reaching the end or taking too long\n",
    "        next_observation: int, the next observation the agent will receive after performing its action\n",
    "        \"\"\"\n",
    "        if terminated:\n",
    "            future_q_value = 0\n",
    "        else:\n",
    "        # TODO: Compute the TD loss for Q-learing\n",
    "            future_q_value = None\n",
    "        td_loss = None\n",
    "        # TODO: Write the update rule for Q(s, a)\n",
    "\n",
    "        #####\n",
    "        self.td_history.append(td_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complete the training loop and run the RL algorithm\n",
    "Fill in the parts of the training loop with the correct methods from the agent. Then, run the cell to complete the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate an agent with learning rate 0.1 and 0.05 exploration rate\n",
    "agent = None\n",
    "#####\n",
    "\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "n_episodes = 30000\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "save_episodes = [0, 1, 5, 10, 50, 100, 200, 500, 1000, 10000]\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    path = [obs]\n",
    "    while not done:\n",
    "        # TODO: Retrieve an action from the agent\n",
    "        action = None\n",
    "        #####\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)  # Let environment process agent's action\n",
    "        \n",
    "        # TODO: Update the agent's policy with environment's feedback\n",
    "        \n",
    "        #####\n",
    "        \n",
    "        # Update bookkeeping variables\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "        path.append(obs)\n",
    "    if episode in save_episodes:\n",
    "        agent.q_history[episode] = (agent.q_values.copy(), path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot the learning metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to plot the training metrics\n",
    "\n",
    "def smooth(signal):\n",
    "    averaging_window_length = 500\n",
    "    return np.convolve(signal, np.ones(averaging_window_length)/averaging_window_length, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Episode rewards')\n",
    "reward = smooth(np.array(env.return_queue).flatten())\n",
    "plt.plot(range(len(reward)), reward)\n",
    "plt.xlabel('Episodes')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Episode lengths')\n",
    "length = smooth(np.array(env.length_queue).flatten())\n",
    "plt.plot(range(len(length)), length)\n",
    "plt.xlabel('Episodes')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Value error')\n",
    "value_error = smooth(agent.td_history)\n",
    "plt.plot(range(len(value_error)), value_error)\n",
    "plt.xlabel('Steps')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot the agent's history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell to define the plotting functions\n",
    "\n",
    "def plot_agent_history(agent):\n",
    "    n = len(agent.q_history)\n",
    "\n",
    "    fig, axs = plt.subplots(n, 2, figsize=(12, 2*n))\n",
    "\n",
    "    for i, (episode, (q_values, path)) in enumerate(agent.q_history.items()):\n",
    "        for ax in axs[i, :]:\n",
    "            setup_axis(ax)\n",
    "        plot_path(axs[i, 0], path, episode)\n",
    "        plot_policy(axs[i, 1], q_values, episode)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def setup_axis(ax):\n",
    "    ax.set_xticks(np.arange(0, 13))\n",
    "    ax.set_yticks(np.arange(0, 4))\n",
    "    ax.set_xlim(0, 12)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.grid(lw=1, c='k')\n",
    "    # Add in color patches\n",
    "    grass = patches.Rectangle(xy=(0, 1), width=13, height=3, fc=\"xkcd:grass green\", alpha=0.6, zorder=0)\n",
    "    start = patches.Rectangle(xy=(0, 0), width=1, height=1, fc=\"xkcd:bright blue\", alpha=0.6, zorder=0)\n",
    "    end = patches.Rectangle(xy=(11, 0), width=1, height=1, fc=\"xkcd:red\", alpha=0.75, zorder=0)\n",
    "    cliff = patches.Rectangle(xy=(1, 0), width=10, height=1, fc=\"xkcd:dark brown\", zorder=2)\n",
    "    for patch in [grass, start, end, cliff]:\n",
    "        ax.add_patch(patch)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "\n",
    "def plot_policy(ax, q_values, episode):\n",
    "    actions = {\n",
    "        0: \"^\",\n",
    "        1: \">\",\n",
    "        2: \"v\",\n",
    "        3: \"<\"\n",
    "    }\n",
    "    policy = q_values.argmax(axis=-1)\n",
    "    xs = np.arange(12)\n",
    "    ys = np.arange(4)\n",
    "    xs, ys = np.meshgrid(xs, ys, indexing=\"xy\")\n",
    "\n",
    "    for x, y, action in zip(xs.flatten(), ys.flatten(), policy[:37]):\n",
    "        ax.scatter(x + 0.5, -y + 3.5, marker=actions[action], c='k')\n",
    "    ax.set_title(f\"Optimal policy after episode {episode}\")\n",
    "\n",
    "\n",
    "def plot_path(ax, path, episode):\n",
    "    path = np.array(path)\n",
    "    xs = path % 12\n",
    "    ys = path // 12\n",
    "\n",
    "    dxs = np.diff(xs)\n",
    "    dys = np.diff(ys)\n",
    "    colours = np.linspace(0, 1, len(dxs))\n",
    "    offsets = np.linspace(-0.2, 0.2, len(dxs))[::-1]\n",
    "    for x, y, dx, dy, c, offset in zip(xs, ys, dxs, dys, colours, offsets):\n",
    "        ax.arrow(\n",
    "            x + 0.5 - offset, -y + 3.5 - offset, 0.9*dx, -0.9*dy,\n",
    "            head_length=0.2, length_includes_head=True,\n",
    "            width=0.1,\n",
    "            fc=cmap(c), alpha=0.5,\n",
    "            zorder=3\n",
    "        )\n",
    "    ax.set_title(f\"Path during episode {episode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agent_history(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
